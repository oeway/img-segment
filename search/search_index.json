{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deep learning with ImJoy \u00b6 We provide a complete framework to perform image segmentation with Deep learning powered by ImJoy . We provide an integrated solution ranging from the creation of training data, over perform the actual training, and applying the trained models on new data. ImJoy \u00b6 ImJoy is image processing platform with an easy to use interface powered by a Python engine running in the background. ImJoy plays a central role in most analysis workflows. We provide links to install the different ImJoy plugins. If you press on the installation link, the ImJoy browser app will open and display a dialog asking if you want to install the specified plugin. To confirm, press the install button. The plugins will be installed in the dedicated workspace segmentation . ImJoy will store this workspace and the plugins in your browser, and the use it another time you have to simply open the browser app and select the appropriate workspace https://imjoy.io/#/app Some of the plugins, e.g. for to train a neural network, require the ImJoy Plugin Engine . You will need to install it only once, but launch it each time you work with ImJoy. For more information for how to install and use the plugin engine, please consult the ImJoy documentation . Segmentation with deep learning \u00b6 We use the popular U-net framework to perform the segmentation task. As other deep learning methods, this requires a training step in order for the model to learn how to segment the data. Once a model is it training it can be applied on new data. We provide pre-trained models. While these model might already perform relatively well on your data, re-training them might further improve their performance. Training \u00b6 In order to train a model images have to be annotated, e.g. the biological structures of interest highlighted in the image. These annotated images can then be used for training. Prediction \u00b6 Once a model is trained, you can apply it on new data.","title":"Summary"},{"location":"#deep-learning-with-imjoy","text":"We provide a complete framework to perform image segmentation with Deep learning powered by ImJoy . We provide an integrated solution ranging from the creation of training data, over perform the actual training, and applying the trained models on new data.","title":"Deep learning with ImJoy"},{"location":"#imjoy","text":"ImJoy is image processing platform with an easy to use interface powered by a Python engine running in the background. ImJoy plays a central role in most analysis workflows. We provide links to install the different ImJoy plugins. If you press on the installation link, the ImJoy browser app will open and display a dialog asking if you want to install the specified plugin. To confirm, press the install button. The plugins will be installed in the dedicated workspace segmentation . ImJoy will store this workspace and the plugins in your browser, and the use it another time you have to simply open the browser app and select the appropriate workspace https://imjoy.io/#/app Some of the plugins, e.g. for to train a neural network, require the ImJoy Plugin Engine . You will need to install it only once, but launch it each time you work with ImJoy. For more information for how to install and use the plugin engine, please consult the ImJoy documentation .","title":"ImJoy"},{"location":"#segmentation-with-deep-learning","text":"We use the popular U-net framework to perform the segmentation task. As other deep learning methods, this requires a training step in order for the model to learn how to segment the data. Once a model is it training it can be applied on new data. We provide pre-trained models. While these model might already perform relatively well on your data, re-training them might further improve their performance.","title":"Segmentation with deep learning"},{"location":"#training","text":"In order to train a model images have to be annotated, e.g. the biological structures of interest highlighted in the image. These annotated images can then be used for training.","title":"Training"},{"location":"#prediction","text":"Once a model is trained, you can apply it on new data.","title":"Prediction"},{"location":"FAQ/","text":"FAQ \u00b6 All good? \u00b6","title":"FAQ"},{"location":"FAQ/#faq","text":"","title":"FAQ"},{"location":"FAQ/#all-good","text":"","title":"All good?"},{"location":"annotation/","text":"Image annotation \u00b6 Annotations are necessary to provide a ground truth for the neural network during training. In this step, the structures of interest, e.g. cell and nuclei, are outline in the images to generate data that will be used to train the neural network. We provide a dedicated plugin to perform this annotation task: the ImageAnnotor. It was developed to integrate seamlessly in the segmentation workflow, and annotation results are stored in the GeoJson file format. Once the data are annotated, different image masks can be generated. These mask are then used together with the raw input images to train the network. Annotations in ImJoy \u00b6 ImJoy provides a dedicated plugin to perform annotations. How to get data inside Different annotation types How to store annotations Explain that in one image, you can have different annotation types. Note that both training and test data have to be annotated Stored as one file annotation.json per folder. As an example, for one of the images this could then look like this: \u251c\u2500 data_for_training/ \u2502 \u251c\u2500 train/ \u2502 \u2502 \u251c\u2500 img1 \u2502 \u2502 \u2502 \u251c\u2500 annotation.json \u2502 \u2502 \u2502 \u251c\u2500 cells.tif \u2502 \u2502 \u2502 \u251c\u2500 nuclei.tif \u2502 \u2502 \u251c\u2500 img2 \u2502 \u2502 \u2502 \u251c\u2500 annotation.json \u2502 \u2502 \u2502 \u251c\u2500 cells.png \u2502 \u2502 \u2502 \u251c\u2500 nuclei.png \u2502 \u2502 ... Convert annotations to mask images \u00b6 Once you have annotated the images, you have to convert these annotations to mask images, which are used as input for the neural network. We provide a dedicated ImJoy plugin to perform this task. Screen shot I allows to specify for each of the annotation types different mask images. We support the following four masks Show examples You can then select either one annotation file, or recursively search a folder for all annotation.json files As an example, for one of the images this could then look like this: \u251c\u2500 data_for_training/ \u2502 \u251c\u2500 train/ \u2502 \u2502 \u251c\u2500 img1 \u2502 \u2502 \u2502 \u251c\u2500 annotation.json \u2502 \u2502 \u2502 \u251c\u2500 cells.tif \u2502 \u2502 \u2502 \u251c\u2500 cells_mask_edge.png \u2502 \u2502 \u2502 \u251c\u2500 nuclei.tif \u2502 \u2502 \u2502 \u251c\u2500 nuclei_mask_full.png","title":"Annotations"},{"location":"annotation/#image-annotation","text":"Annotations are necessary to provide a ground truth for the neural network during training. In this step, the structures of interest, e.g. cell and nuclei, are outline in the images to generate data that will be used to train the neural network. We provide a dedicated plugin to perform this annotation task: the ImageAnnotor. It was developed to integrate seamlessly in the segmentation workflow, and annotation results are stored in the GeoJson file format. Once the data are annotated, different image masks can be generated. These mask are then used together with the raw input images to train the network.","title":"Image annotation"},{"location":"annotation/#annotations-in-imjoy","text":"ImJoy provides a dedicated plugin to perform annotations. How to get data inside Different annotation types How to store annotations Explain that in one image, you can have different annotation types. Note that both training and test data have to be annotated Stored as one file annotation.json per folder. As an example, for one of the images this could then look like this: \u251c\u2500 data_for_training/ \u2502 \u251c\u2500 train/ \u2502 \u2502 \u251c\u2500 img1 \u2502 \u2502 \u2502 \u251c\u2500 annotation.json \u2502 \u2502 \u2502 \u251c\u2500 cells.tif \u2502 \u2502 \u2502 \u251c\u2500 nuclei.tif \u2502 \u2502 \u251c\u2500 img2 \u2502 \u2502 \u2502 \u251c\u2500 annotation.json \u2502 \u2502 \u2502 \u251c\u2500 cells.png \u2502 \u2502 \u2502 \u251c\u2500 nuclei.png \u2502 \u2502 ...","title":"Annotations in ImJoy"},{"location":"annotation/#convert-annotations-to-mask-images","text":"Once you have annotated the images, you have to convert these annotations to mask images, which are used as input for the neural network. We provide a dedicated ImJoy plugin to perform this task. Screen shot I allows to specify for each of the annotation types different mask images. We support the following four masks Show examples You can then select either one annotation file, or recursively search a folder for all annotation.json files As an example, for one of the images this could then look like this: \u251c\u2500 data_for_training/ \u2502 \u251c\u2500 train/ \u2502 \u2502 \u251c\u2500 img1 \u2502 \u2502 \u2502 \u251c\u2500 annotation.json \u2502 \u2502 \u2502 \u251c\u2500 cells.tif \u2502 \u2502 \u2502 \u251c\u2500 cells_mask_edge.png \u2502 \u2502 \u2502 \u251c\u2500 nuclei.tif \u2502 \u2502 \u2502 \u251c\u2500 nuclei_mask_full.png","title":"Convert annotations to mask images"},{"location":"cell-segmentation/","text":"Cell segmentation \u00b6 Setting up the Training \u00b6 Parameters Retraining a model with your own data Post-processing \u00b6 The A-net plugin predicts mask for the different fluorescent channels. To obtain individual cells or nuclei, we provide a dedicate plugin to perform this step.","title":"Segmentation"},{"location":"cell-segmentation/#cell-segmentation","text":"","title":"Cell segmentation"},{"location":"cell-segmentation/#setting-up-the-training","text":"Parameters Retraining a model with your own data","title":"Setting up the Training"},{"location":"cell-segmentation/#post-processing","text":"The A-net plugin predicts mask for the different fluorescent channels. To obtain individual cells or nuclei, we provide a dedicate plugin to perform this step.","title":"Post-processing"},{"location":"data-organization/","text":"Data organization \u00b6 To perform training, you have to provide both the input images as well as target images (the masks created from your annotations). These data have to be organized according to following guidelines. Note that this strict organization is ONLY needed for training, for prediction, data can be organized more freely. Data has to be stored in two folders train , valid . The train folder will be used to train the neural network, the valid folder to continuously monitor how well the training worked. Both folders have to contain images and annotations. Sub-folders then stored different field of views. Images can be stored either as multi-channel or mono-channel, but they must have the same name across In an example, this could look like the organization shown below. For each \u251c\u2500 data_for_training/ \u2502 \u251c\u2500 train/ \u2502 \u2502 \u251c\u2500 img1 \u2502 \u2502 \u2502 \u251c\u2500 cells.png \u2502 \u2502 \u2502 \u251c\u2500 nuclei.png \u2502 \u2502 \u251c\u2500 img2 \u2502 \u2502 \u2502 \u251c\u2500 cells.png \u2502 \u2502 \u2502 \u251c\u2500 nuclei.png \u2502 \u2502 ... \u2502 \u251c\u2500 valid/ \u2502 \u2502 \u251c\u2500 img57 \u2502 \u2502 \u2502 \u251c\u2500 cells.png \u2502 \u2502 \u2502 \u251c\u2500 nuclei.png \u2502 \u2502 \u251c\u2500 img58 \u2502 \u2502 \u2502 \u251c\u2500 cells.png \u2502 \u2502 \u2502 \u251c\u2500 nuclei.png \u2502 \u2502 ... How many cells for training an validation? \u00b6 There is no simple rule for how many images / annotated cells or nuclei you will need to obtain good results. As an example, for standard segmentation of adherent cells, we obtained good results with a training set of 5 images (with up to 10-15 cells per image), and test set of 2 images. For more challenging data-sets, you can add more training data if you see that the performance is not satisfying with the current training data set.","title":"Data organization"},{"location":"data-organization/#data-organization","text":"To perform training, you have to provide both the input images as well as target images (the masks created from your annotations). These data have to be organized according to following guidelines. Note that this strict organization is ONLY needed for training, for prediction, data can be organized more freely. Data has to be stored in two folders train , valid . The train folder will be used to train the neural network, the valid folder to continuously monitor how well the training worked. Both folders have to contain images and annotations. Sub-folders then stored different field of views. Images can be stored either as multi-channel or mono-channel, but they must have the same name across In an example, this could look like the organization shown below. For each \u251c\u2500 data_for_training/ \u2502 \u251c\u2500 train/ \u2502 \u2502 \u251c\u2500 img1 \u2502 \u2502 \u2502 \u251c\u2500 cells.png \u2502 \u2502 \u2502 \u251c\u2500 nuclei.png \u2502 \u2502 \u251c\u2500 img2 \u2502 \u2502 \u2502 \u251c\u2500 cells.png \u2502 \u2502 \u2502 \u251c\u2500 nuclei.png \u2502 \u2502 ... \u2502 \u251c\u2500 valid/ \u2502 \u2502 \u251c\u2500 img57 \u2502 \u2502 \u2502 \u251c\u2500 cells.png \u2502 \u2502 \u2502 \u251c\u2500 nuclei.png \u2502 \u2502 \u251c\u2500 img58 \u2502 \u2502 \u2502 \u251c\u2500 cells.png \u2502 \u2502 \u2502 \u251c\u2500 nuclei.png \u2502 \u2502 ...","title":"Data organization"},{"location":"data-organization/#how-many-cells-for-training-an-validation","text":"There is no simple rule for how many images / annotated cells or nuclei you will need to obtain good results. As an example, for standard segmentation of adherent cells, we obtained good results with a training set of 5 images (with up to 10-15 cells per image), and test set of 2 images. For more challenging data-sets, you can add more training data if you see that the performance is not satisfying with the current training data set.","title":"How many cells for training an validation?"},{"location":"licence/","text":"License \u00b6 MIT License Copyright \u00a9 Wei Ouyang, Florian Mueller Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Licence"},{"location":"licence/#license","text":"MIT License Copyright \u00a9 Wei Ouyang, Florian Mueller Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"prediction/","text":"Prediction \u00b6 Once a model is trained, you can apply it to a larger data-set. Here, the data does not need to be organized in a strict way. We support currently the following possibilities, others can be devised if necessary.","title":"Prediction"},{"location":"prediction/#prediction","text":"Once a model is trained, you can apply it to a larger data-set. Here, the data does not need to be organized in a strict way. We support currently the following possibilities, others can be devised if necessary.","title":"Prediction"},{"location":"projection/","text":"2D projections \u00b6 Image segmentation is performed in 2D. 3D images thus have to transformed into 2D images with a projection along Z. We provide a plugin to perform such projections, where we provide different approaches. The easiest method to achieve is the so-called maximum intensity projection (MIP) , where for each XY position the highest pixel value along the z-axis is used.","title":"2D projection"},{"location":"projection/#2d-projections","text":"Image segmentation is performed in 2D. 3D images thus have to transformed into 2D images with a projection along Z. We provide a plugin to perform such projections, where we provide different approaches. The easiest method to achieve is the so-called maximum intensity projection (MIP) , where for each XY position the highest pixel value along the z-axis is used.","title":"2D projections"},{"location":"release-notes/","text":"Release Notes \u00b6 Upgrading \u00b6 Maintenance team \u00b6 The current and past team members. @oeway @muellerflorian Version \u00b6 Initial release","title":"Release Notes"},{"location":"release-notes/#release-notes","text":"","title":"Release Notes"},{"location":"release-notes/#upgrading","text":"","title":"Upgrading"},{"location":"release-notes/#maintenance-team","text":"The current and past team members. @oeway @muellerflorian","title":"Maintenance team"},{"location":"release-notes/#version","text":"Initial release","title":"Version"},{"location":"workflows/","text":"Workflows \u00b6 For each workflow, we provide an installation link for ImJoy to generate a dedicated workspace with all necessary plugin, and the possibility to load a pre-trained model. This model can then be applied to test data to familiarize yourself with our approach. The pre-trained model can also be used to initialize training on your own data. This process (also called transfer learning ) tends to reduce the time for training considerably, For each workflow we provide a link to example data. These folders contain Data with the annotations (folders train , valid ), and some test data (folder test ). These folders can be processed with the AnnotationGenerator plugin. Processed data with the annotations being converted to images (folder unet_data ). This folder can be used with the Anet-Lite plugin. The folder structure for the segmentation of the cell membrane looks like This . \u251c\u2500 anet/ # Folder for Anet plugin \u2502 \u251c\u2500 train \u2502 \u2502 \u251c\u2500 img1/ \u2502 \u2502 \u2502 \u251c\u2500 cells.png \u2502 \u2502 \u2502 \u251c\u2500 cells_mask_edge.png \u2502 \u2502 \u251c\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500 valid \u2502 \u2502 \u251c\u2500 img10/ \u2502 \u2502 \u2502 \u251c\u2500 cells.png \u2502 \u2502 \u2502 \u251c\u2500 cells_mask_edge.png \u2502 \u2502 \u251c\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500 test1 \u2502 \u2502 \u251c\u2500 C3-img21.tif \u2502 \u2502 \u251c\u2500 ... \u2502 \u251c\u2500 test/ # Folder for AnnotationGenerator \u2502 \u251c\u2500 C3-img20.tif \u2502 \u251c\u2500 ... \u2502 \u251c\u2500 train/ # Folder for AnnotationGenerator \u2502 \u251c\u2500 C3-img1.tif \u2502 \u251c\u2500 C3-img1__RoiSet.tif \u2502 \u251c\u2500 ... \u2502 \u251c\u2500 valid/ # Folder for AnnotationGenerator \u2502 \u251c\u2500 C3-img10.tif \u2502 \u251c\u2500 C3-img10__RoiSet.tif \u2502 \u251c\u2500 ... . Segmentation of cell membrane \u00b6 This workflow allows to segment cellular membranes. The provided examples is for cell cortex in the developing drosophila embryo. TODO : SHOW EXAMPLE IMAGE We provide some test data for a segmentation of the on Dropbox . Data consists of a GFP stains of the membranes (e.g. C3-img1.tif ) and the corresponding annotations from FIJI (e.g. C3-img1__RoiSet.zip ) TODO should be provided as a Release. Should we pro AnnotationGenerator: TODO Link for ImJoy Segmentation of cells and nuclei \u00b6 This workflow allows to segment cells and nuclei membranes. The provided examples is for CellMask stain of cells, and a DAPI stain of nuclei. cell cortex in the developing drosophila embryo. TODO : SHOW EXAMPLE IMAGE AnnotationGenerator: TODO Link for ImJoy","title":"Workflows"},{"location":"workflows/#workflows","text":"For each workflow, we provide an installation link for ImJoy to generate a dedicated workspace with all necessary plugin, and the possibility to load a pre-trained model. This model can then be applied to test data to familiarize yourself with our approach. The pre-trained model can also be used to initialize training on your own data. This process (also called transfer learning ) tends to reduce the time for training considerably, For each workflow we provide a link to example data. These folders contain Data with the annotations (folders train , valid ), and some test data (folder test ). These folders can be processed with the AnnotationGenerator plugin. Processed data with the annotations being converted to images (folder unet_data ). This folder can be used with the Anet-Lite plugin. The folder structure for the segmentation of the cell membrane looks like This . \u251c\u2500 anet/ # Folder for Anet plugin \u2502 \u251c\u2500 train \u2502 \u2502 \u251c\u2500 img1/ \u2502 \u2502 \u2502 \u251c\u2500 cells.png \u2502 \u2502 \u2502 \u251c\u2500 cells_mask_edge.png \u2502 \u2502 \u251c\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500 valid \u2502 \u2502 \u251c\u2500 img10/ \u2502 \u2502 \u2502 \u251c\u2500 cells.png \u2502 \u2502 \u2502 \u251c\u2500 cells_mask_edge.png \u2502 \u2502 \u251c\u2500 ... \u2502 \u2502 \u2502 \u251c\u2500 test1 \u2502 \u2502 \u251c\u2500 C3-img21.tif \u2502 \u2502 \u251c\u2500 ... \u2502 \u251c\u2500 test/ # Folder for AnnotationGenerator \u2502 \u251c\u2500 C3-img20.tif \u2502 \u251c\u2500 ... \u2502 \u251c\u2500 train/ # Folder for AnnotationGenerator \u2502 \u251c\u2500 C3-img1.tif \u2502 \u251c\u2500 C3-img1__RoiSet.tif \u2502 \u251c\u2500 ... \u2502 \u251c\u2500 valid/ # Folder for AnnotationGenerator \u2502 \u251c\u2500 C3-img10.tif \u2502 \u251c\u2500 C3-img10__RoiSet.tif \u2502 \u251c\u2500 ... .","title":"Workflows"},{"location":"workflows/#segmentation-of-cell-membrane","text":"This workflow allows to segment cellular membranes. The provided examples is for cell cortex in the developing drosophila embryo. TODO : SHOW EXAMPLE IMAGE We provide some test data for a segmentation of the on Dropbox . Data consists of a GFP stains of the membranes (e.g. C3-img1.tif ) and the corresponding annotations from FIJI (e.g. C3-img1__RoiSet.zip ) TODO should be provided as a Release. Should we pro AnnotationGenerator: TODO Link for ImJoy","title":"Segmentation of cell membrane"},{"location":"workflows/#segmentation-of-cells-and-nuclei","text":"This workflow allows to segment cells and nuclei membranes. The provided examples is for CellMask stain of cells, and a DAPI stain of nuclei. cell cortex in the developing drosophila embryo. TODO : SHOW EXAMPLE IMAGE AnnotationGenerator: TODO Link for ImJoy","title":"Segmentation of cells and nuclei"}]}